{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "painted-madness",
   "metadata": {},
   "source": [
    "# Анализ геолокации фотографий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "champion-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "previous-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-value",
   "metadata": {},
   "source": [
    "## Сбор данных\n",
    "\n",
    "vk.com позволяет выгружать список фотографий, сделанных в радиусе r от какой-то точки с помощью метода API photos.search\n",
    "\n",
    "https://vk.com/dev/photos.search\n",
    "\n",
    "Для осуществления запростов нужен service token, который достаточно легко получить (https://vk.com/dev/access_token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "traditional-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert service token\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "token = getpass.getpass('Insert service token\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-agent",
   "metadata": {},
   "source": [
    "Запросы посылаем распространенной библиотекой для requests.\n",
    "\n",
    "Получаемый ответ для удобства сразу конвертируем в JSON.\n",
    "Получаем что-то вроде такого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "united-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"response\": {\n",
      "  \"count\": 27554,\n",
      "  \"items\": [\n",
      "   {\n",
      "    \"album_id\": -7,\n",
      "    \"date\": 1615270447,\n",
      "    \"id\": 457242496,\n",
      "    \"owner_id\": -175649192,\n",
      "    \"has_tags\": false,\n",
      "    \"lat\": 54.57587,\n",
      "    \"long\": 33.184714,\n",
      "    \"text\": \"\",\n",
      "    \"user_id\": 100\n",
      "   }\n",
      "  ]\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "lat  = 54.5008\n",
    "long = 33.0088\n",
    "response = requests.get(\n",
    "                'https://api.vk.com/method/photos.search?lat={}&long={}&count=1&v=5.130&access_token={}'.\\\n",
    "            format(lat, long, token)\n",
    "            )\n",
    "response = json.loads(response.text)\n",
    "\n",
    "# удалим из ответа ссылки на изображение в разных разрешениях\n",
    "# ссылки занимают много места и не позволяют с ходу поянть структуру полученных данных \n",
    "del response['response']['items'][0]['sizes']\n",
    "\n",
    "print(json.dumps(response, indent = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-flooring",
   "metadata": {},
   "source": [
    "Видно, что мы можем без проблем получить:\n",
    "\n",
    " - координаты фотографии\n",
    " - дату и время, когда она была загружена *(или сделана, тут не понятно)*\n",
    " - id пользователя, который её заргузил\n",
    " - текстовое описание, если пользователь его добавил\n",
    " - еще несколько атрибутов, которые нас не сильно инетересуют"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-tucson",
   "metadata": {},
   "source": [
    "### Генерируем точки из которых будем собирать фото\n",
    "\n",
    "Для сбора данных необходим список интересующих нас точек, в окрестности которых мы и будем искать фотографии.\n",
    "Пишем функцию, которая генерирует набор точек, лежащих в интересующем нас прямоугольнике.\n",
    "\n",
    "На вход функции будем подавать координаты *(широта, долгота)* верхней левой и нижней правой вершины прямоугольника и шаг с которым в нём будут располагаться точки.\n",
    "\n",
    "*Для простоты Землю считаем идеальной сферой.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noble-courtesy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78583.13010355463\n",
      "8788\n"
     ]
    }
   ],
   "source": [
    "def get_points(top_left, bot_right, step):\n",
    "    step = 2*step\n",
    "    long1, lat1 = top_left\n",
    "    long2, lat2 = bot_right\n",
    "    earth_r = 6371000 #m\n",
    "  \n",
    "    if abs(long1-long2)<5:\n",
    "        long = np.pi/180*(long1+long2)/2\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'The input area is too big. Difference in longitude is expected to be less than 5 degrees'\n",
    "            )\n",
    "        \n",
    "    one_degree_lat_length = 2*np.pi*earth_r*np.cos(long)/360 #m\n",
    "    one_degree_long_length = 2*np.pi*earth_r/360 #m\n",
    "    \n",
    "    print(one_degree_lat_length)\n",
    "    \n",
    "    one_degree_lat_length = 78847 #m\n",
    "    one_degree_long_length = 110500 #m\n",
    "    \n",
    "    lat_step  = step / one_degree_lat_length\n",
    "    long_step = 1.5*step/np.cos(np.pi/6) / one_degree_long_length\n",
    "    \n",
    "    lat = np.arange(lat1, lat2, lat_step)\n",
    "    long = np.arange(long2, long1, long_step) \n",
    "    xx, yy = np.meshgrid(long, lat)\n",
    "    out_even = np.stack((xx.flatten(),yy.flatten()), axis=-1)\n",
    "    \n",
    "    lat = np.arange(lat1+lat_step/2, lat2, lat_step)\n",
    "    long = np.arange(long2+long_step/2, long1, long_step)\n",
    "    xx, yy = np.meshgrid(long, lat)\n",
    "    out_odd = np.stack((xx.flatten(),yy.flatten()), axis=-1)\n",
    "\n",
    "    out = np.concatenate((out_even, out_odd), axis=0)\n",
    "        \n",
    "    print(len(out))\n",
    "    return out\n",
    "\n",
    "top_left = (45.0984557078117, 41.87632533877263)\n",
    "bot_right = (44.96500662165554, 42.08675340577944)\n",
    "points = get_points(top_left, bot_right, 90)\n",
    "\n",
    "points = pd.DataFrame(points)\n",
    "points.columns = ['lat', 'long']\n",
    "points.to_csv(\"points.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-greek",
   "metadata": {},
   "source": [
    "Пример того, как выглядит набор кругов с центрами в сгенерированных точках\n",
    "\n",
    "![Title](imgs/circles.png)\n",
    "\n",
    "После того, как мы получили набор точек, можно начинать скачивать всё что мы пожелаем.\n",
    "\n",
    "![Title](imgs/TP_ready.gif)\n",
    "\n",
    "Сначала - пишем функцию, скачивающую все фотографии в заданном радиусе от одной точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photos_from_location(long, lat, radius, token, log, error):\n",
    "    r = requests.get(\n",
    "                'https://api.vk.com/method/photos.search?long={}&lat={}&count={}&start_time=1230757200&radius={}&v=5.130&access_token={}'.\\\n",
    "            format(lat, long, 0, radius, token)\n",
    "            ) \n",
    "    if 'error' in r.text:\n",
    "        print('error1')\n",
    "        error.loc[len(error.index)] = [long,lat,r.text]\n",
    "        return None\n",
    "    num_photos = json.loads(r.text)['response']['count']\n",
    "#     print(num_photos)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    count = 1000\n",
    "    photos_downloaded = 0\n",
    "    new = pd.DataFrame()\n",
    "\n",
    "    # костыльный while, который необходим, так как vk выдает только часть фотографий\n",
    "    # поэтому запоминаем сколько фото у нас уже есть и просим выдать список с соответствующим offset'ом\n",
    "    # до тех пор пока не скачаем все или пока vk не перестанет нам что-то отдавать\n",
    "    while photos_downloaded < num_photos:\n",
    "        r = requests.get(\n",
    "                'https://api.vk.com/method/photos.search?long={}&lat={}&count={}&offset={}&start_time=1230757200&radius={}&v=5.130&access_token={}'.\\\n",
    "            format(lat, long, count, photos_downloaded, radius, token)\n",
    "            )\n",
    "        if 'error' in r.text:\n",
    "            print('error2')\n",
    "            error.loc[len(error.index)] = [long,lat,r.text]\n",
    "            return None\n",
    "    \n",
    "        res = json.loads(r.text)\n",
    "        n = len(res['response']['items'])\n",
    "        if n == 0:\n",
    "            break\n",
    "#         print('n', n)\n",
    "        new = pd.DataFrame(res['response']['items'])\n",
    "        new = new.astype({'sizes': 'str'})\n",
    "\n",
    "        df = df.append(new, ignore_index=True)        \n",
    "        photos_downloaded += n\n",
    "    log.loc[len(log.index)] = [long,lat,num_photos,df.shape[0]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-parks",
   "metadata": {},
   "source": [
    "Функция `get_photos_from_location`, помимо того что собирает информцию о фотографиях в заданном радиусе от нужной точки, еще и логирует ошибки и то, как много фотографий удалось получить.\n",
    "\n",
    "Затем - функция, которая ходит по массиву точек и собирает все что можно с помощью уже готовой `get_photos_from_location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photos(points, radius, token, loud=True):\n",
    "    start = time.time()\n",
    "    \n",
    "    df = pd.DataFrame({'date':[], 'id':[], 'owner_id':[], 'lat':[], 'long':[], 'sizes':[], 'text':[]})\n",
    "    log = pd.DataFrame({'long':[],'lat':[],'num_photos':[],'photos_downloaded':[]})\n",
    "    error = pd.DataFrame({'long':[],'lat':[],'err_msg':[]})\n",
    "    df.to_csv(\"/media/drev/DISK/VirtualMachine/data_backup.csv\", mode='a')\n",
    "    log.to_csv(\"/media/drev/DISK/VirtualMachine/log.csv\", mode='a')\n",
    "    error.to_csv(\"/media/drev/DISK/VirtualMachine/err.csv\", mode='a')\n",
    "    \n",
    "    for n, point in enumerate(points):\n",
    "        long, lat = point\n",
    "#         print(long, lat)\n",
    "        data = get_photos_from_location(long, lat, radius, token, log, error)\n",
    "        if n % 100 == 0:\n",
    "            if 'post_id' in df.columns:\n",
    "                df.drop(columns=['post_id'], inplace=True)\n",
    "            if 'album_id' in df.columns:\n",
    "                df.drop(columns=['album_id'], inplace=True)\n",
    "            if 'has_tags' in df.columns:\n",
    "                df.drop(columns=['has_tags'], inplace=True)\n",
    "            if 'user_id' in df.columns:\n",
    "                df.drop(columns=['user_id'], inplace=True)\n",
    "            \n",
    "            print('{:.1f}% complete\\t{}/{}\\t{:.2e} sec passed\\t{:.2e} sec left'\\\n",
    "                  .format(n/len(points)*100,n,len(points),time.time()-start,(time.time()-start)/(n+1)*(len(points)-n)))\n",
    "            df.to_csv(\"data_backup.csv\", mode='a', header=False)\n",
    "            log.to_csv(\"log.csv\", mode='a', header=False)\n",
    "            error.to_csv(\"err.csv\", mode='a', header=False)\n",
    "            \n",
    "            df = pd.DataFrame()\n",
    "            log = pd.DataFrame({'long':[],'lat':[],'num_photos':[],'photos_downloaded':[]})\n",
    "            error = pd.DataFrame({'long':[],'lat':[],'err_msg':[]})\n",
    "            continue\n",
    "        if data is None:\n",
    "            continue\n",
    "        df = df.append(data, ignore_index=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-float",
   "metadata": {},
   "source": [
    "Так как порой процесс парсинга сильно затягивается, потерять драгоценные биты, полученные в течение часов работы очень не хочется - каждые n шагов сохрняем всё добро в файл. Это помимо прочего позволяет не забивать оперативную память.\n",
    "\n",
    "Берем в качестве примера замечательный Ставрополь, раскидываем наши круги радиусом 800 метров и смотрим что из этого всего выйдет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_left = (45.0984557078117, 41.87632533877263)\n",
    "bot_right = (44.96500662165554, 42.08675340577944)\n",
    "radius = 800\n",
    "\n",
    "points = get_points(top_left, bot_right, radius)\n",
    "get_photos(points, radius, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-romantic",
   "metadata": {},
   "source": [
    "Ждем некоторое колличество времени и получаем файлик со всем всей необходимой информацией.\n",
    "\n",
    "Из любопытства посмотрим на гистограмму распределения числа фотографий в точке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-glenn",
   "metadata": {},
   "source": [
    "### Проблема"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-bachelor",
   "metadata": {},
   "source": [
    "Из гистограммы видим что из каждой точки, api vk отдает отдает не больше 3000 фотографий. По этой причине мы теряем большую часть данных, что является критической проблемой.\n",
    "\n",
    "Проблему решаем в лоб: ищем точки из которых удалось выгрузить не всё, даелаем разбивку с меньшим шагом и снова все скачиваем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(\"log.csv\")\n",
    "\n",
    "log['num_photos'] = log['num_photos'].astype(float).astype(int)\n",
    "log['photos_downloaded'] = log['photos_downloaded'].astype(float).astype(int)\n",
    "\n",
    "out['num_lost'] = log['num_photos']-log['photos_downloaded']\n",
    "out = log.loc[log[''num_lost'']>10]\n",
    "\n",
    "out.sort_values(by='num_lost', ascending=False, inplace=True)\n",
    "print(out['num_lost'].sum())\n",
    "\n",
    "one_degree_long_length = 110500 #m\n",
    "one_degree_lat_length = 2*np.pi*6371000*np.cos(np.pi/180*45.04591029)/360 #m\n",
    "\n",
    "print(one_degree_lat_length)\n",
    "\n",
    "fine_points = np.empty((0,2))\n",
    "print(fine_points)\n",
    "\n",
    "for i in range(len(out)):\n",
    "    lat, long = out[i:i+1]['long'].values[0], out[i:i+1]['lat'].values[0]\n",
    "#     print(long,lat)\n",
    "    r = 100\n",
    "    top_left = (long+r/one_degree_long_length, lat-r/one_degree_lat_length)\n",
    "    bot_right = (long-r/one_degree_long_length, lat+r/one_degree_lat_length)\n",
    "    fine_points = np.concatenate((fine_points, get_points(top_left, bot_right, 20, dense=False)))\n",
    "# print(fine_points[:17])\n",
    "print(fine_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-territory",
   "metadata": {},
   "source": [
    "Так за несколько итераций, в последней из которых ходили с шагом 10 метров, удалось скачать практически все доступные фотографии с гелокацией из инетересущей области.\n",
    "\n",
    "Итого получили ХХХ фотографий, что кажется весьма неплохо."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-mixture",
   "metadata": {},
   "source": [
    "## Чистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-storm",
   "metadata": {},
   "source": [
    "Весь огромный массив фотографий полон шума и от которого его необходимо очистить.\n",
    "\n",
    "Для начала - очевидное: удаляем дубликаты, которые появились из-за пересечения кругов или неточности с которой vk измеряет расстояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"/media/drev/DISK/VirtualMachine/data_backup_2.csv\")\n",
    "print(data1.shape)\n",
    "\n",
    "data_clean = data1\n",
    "\n",
    "data_clean.drop(columns=['sizes','text'], inplace=True)\n",
    "data_clean = data_clean.drop_duplicates()\n",
    "data_clean.reset_index(inplace=True)\n",
    "\n",
    "print(data_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-psychology",
   "metadata": {},
   "source": [
    "Осталось ХХХ фотографий.\n",
    "\n",
    "Избавляемся от тех, у которых отсутствуют координаты или дата.\n",
    "Помимо прочего, удалем залетные фоторгафии, у которых координаты лежат вне указанной нами области:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = data_clean[data_clean['long'].isna()].index\n",
    "data_clean.drop(index_names, inplace=True)\n",
    "\n",
    "index_names = data_clean[data_clean['lat'].isna()].index\n",
    "data_clean.drop(index_names, inplace=True)\n",
    "\n",
    "index_names = data_clean[data_clean['date'].isna()].index\n",
    "data_clean.drop(index_names, inplace=True)\n",
    "\n",
    "index_names = data_clean[data_clean['long']=='long'].index\n",
    "data_clean.drop(index_names, inplace=True)\n",
    "\n",
    "data_clean['long'] = data_clean['long'].astype(float)\n",
    "data_clean['lat'] = data_clean['lat'].astype(float)\n",
    "\n",
    "index_names = data_clean[data_clean['long']<top_left[1]].index\n",
    "data_clean.drop(index_names, inplace=True)\n",
    "index_names = data_clean[data_clean['long']>bot_right[1]].index\n",
    "data_clean.drop(index_names, inplace=True)\n",
    "\n",
    "index_names = data_clean[data_clean['lat']<bot_right[0]].index\n",
    "data_clean.drop(index_names, inplace=True)\n",
    "index_names = data_clean[data_clean['lat']>top_left[0]].index\n",
    "data_clean.drop(index_names, inplace=True)\n",
    "\n",
    "data_clean['date'] = pd.to_datetime(data_clean['date'], unit='s')\n",
    "\n",
    "data_clean.to_csv(\"data_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-calendar",
   "metadata": {},
   "source": [
    "Чистые данные сохраняем.\n",
    "\n",
    "Теперь построим все на карте. Для этого хорошо подходит сервис kepler.gl, который сделал Uber для визуализации поездок такси. (ссылка)\n",
    "\n",
    "Сервис бесплатный, функциональный и без проблем справляется с большими объемами данных\n",
    "\n",
    "![Гистограмма из kepler](imgs/kepler_hist1.png)\n",
    "\n",
    "Вот такая красота у нас получилась.\n",
    "\n",
    "Бросаются в глаза аномальные высоченные пики, расположенные далеко не в центре города. После внимательного изучения стало ясно, что эти пики соответствуют положению особо активных пользователей, которые публикуют оочень много фотографий. Как правило это онлайн-магазины которые загружают пачки фотографий продаваемых товаров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-fields",
   "metadata": {},
   "source": [
    "### Удаляем посты от отдельных юзеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 200\n",
    "data['lat_bin'] = pd.qcut(data['lat'], n_bins, labels=False)\n",
    "data['long_bin'] = pd.qcut(data['long'], n_bins, labels=False)\n",
    "\n",
    "data['feature'] = data['owner_id'].astype(str) + '_' + data['long_bin'].astype(str) + '_' + data['long_bin'].astype(str)\n",
    "\n",
    "data_filtered = data.loc[~data['feature'].duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-burden",
   "metadata": {},
   "source": [
    "## Анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-coordination",
   "metadata": {},
   "source": [
    "### Ищем интересные места"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-dollar",
   "metadata": {},
   "source": [
    "### Ищем особенности в данных в зависимости от даты/ времени суток/ времени года"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
